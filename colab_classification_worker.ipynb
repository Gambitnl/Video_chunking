{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D&D Session IC/OOC Classification Worker\n",
    "\n",
    "This notebook runs on Google Colab to provide GPU-accelerated classification for the VideoChunking pipeline.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Open in Colab**: `File -> Open in Colab` or upload to Google Drive\n",
    "2. **Enable GPU**: `Runtime -> Change runtime type -> Hardware accelerator -> GPU (T4)`\n",
    "3. **Locate Google Drive**: Run Cell 1\n",
    "4. **Install Dependencies**: Run Cell 2\n",
    "5. **Load Model**: Run Cell 3 (this may take a few minutes)\n",
    "6. **Start Worker**: Run Cell 4 - this will continuously process jobs\n",
    "\n",
    "## How It Works\n",
    "\n",
    "- Your local pipeline uploads classification jobs to `VideoChunking/classification_pending/` in Google Drive\n",
    "- This notebook watches that folder and processes jobs using a local LLM\n",
    "- Results are written to `VideoChunking/classification_complete/`\n",
    "- Your local pipeline polls for results and continues\n",
    "\n",
    "Keep this notebook running while processing sessions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Locate Google Drive (Works locally AND in Colab)\nimport os\nimport sys\n\n# Detect if we're ACTUALLY in Colab (not just VS Code Colab integration)\n# Real Colab has /content directory AND is on Linux\nIN_COLAB = False\n\ntry:\n    from google.colab import drive\n    # Real Colab characteristics:\n    # - Has /content directory\n    # - Platform is Linux (not Windows/nt)\n    # - sys.platform is 'linux'\n    if os.path.exists(\"/content\") and sys.platform.startswith(\"linux\"):\n        IN_COLAB = True\n        print(\"[INFO] Detected real Google Colab environment\")\n    else:\n        # VS Code Colab integration on Windows - treat as local\n        drive = None\n        print(f\"[INFO] VS Code Colab integration detected (platform: {sys.platform})\")\nexcept ImportError:\n    drive = None\n\ndrive_root = None\n\nif IN_COLAB:\n    # Running in actual Google Colab - mount Drive\n    print(\"[INFO] Mounting Google Drive in Colab...\")\n    drive.mount(\"/content/drive\")\n    drive_root = \"/content/drive/MyDrive\"\nelse:\n    # Running locally (or VS Code Colab integration) - find Google Drive Desktop sync folder\n    print(\"[INFO] Running locally - looking for Google Drive Desktop sync folder\")\n    \n    # Use raw string paths to avoid PosixPath issues in WSL/containers\n    drive_root_candidates = [\n        r\"G:\\My Drive\",\n        os.path.join(os.path.expanduser(\"~\"), \"My Drive\"),\n        os.path.join(os.path.expanduser(\"~\"), \"Google Drive\"),\n    ]\n    \n    # Find first existing path\n    for candidate in drive_root_candidates:\n        if os.path.exists(candidate):\n            drive_root = candidate\n            print(f\"[OK] Found Google Drive at: {drive_root}\")\n            break\n    \n    if drive_root is None:\n        raise RuntimeError(\n            \"Could not find Google Drive sync folder. \"\n            \"Make sure Google Drive Desktop is installed and syncing. \"\n            f\"Checked: {drive_root_candidates}\"\n        )\n\n# Use string paths with os.path.join (more reliable than pathlib)\npending_dir = os.path.join(drive_root, \"VideoChunking\", \"classification_pending\")\ncomplete_dir = os.path.join(drive_root, \"VideoChunking\", \"classification_complete\")\n\n# Create directories if they don't exist\nos.makedirs(pending_dir, exist_ok=True)\nos.makedirs(complete_dir, exist_ok=True)\n\nprint(f\"[OK] Pending jobs: {pending_dir}\")\nprint(f\"[OK] Completed jobs: {complete_dir}\")\n\n# Verify paths are string types (not PosixPath/WindowsPath)\nassert isinstance(pending_dir, str), f\"pending_dir should be str, got {type(pending_dir)}\"\nassert isinstance(complete_dir, str), f\"complete_dir should be str, got {type(complete_dir)}\"\nprint(f\"[OK] Path types verified: str\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "!pip install -q transformers torch accelerate bitsandbytes\n",
    "\n",
    "print(\"[OK] Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Load LLM Model (Local version - no quantization needed)\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Model selection - Using a smaller model that works locally without quantization\n# Options: Qwen/Qwen2.5-3B-Instruct, Qwen/Qwen2.5-1.5B-Instruct (even smaller)\nMODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Smaller for local CPU/GPU\n\nprint(f\"Loading model: {MODEL_NAME}\")\nprint(\"This may take 2-5 minutes on first run...\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load model (will auto-detect CUDA if available)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True\n)\n\nif device == \"cpu\":\n    model = model.to(device)\n\n# Print memory usage\nif torch.cuda.is_available():\n    print(f\"[OK] Model loaded on GPU\")\n    print(f\"[INFO] GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\nelse:\n    print(f\"[OK] Model loaded on CPU\")\n    print(f\"[WARN] Running on CPU will be slower but will work fine for classification\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Classification Functions\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "def classify_segment(prompt: str, max_length: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Classify a single segment using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Classification prompt\n",
    "        max_length: Maximum tokens for generation\n",
    "    \n",
    "    Returns:\n",
    "        Model response text\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def build_prompt(segment_data: Dict, job_data: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Build classification prompt from segment and job data.\n",
    "    \n",
    "    Args:\n",
    "        segment_data: Current segment with text\n",
    "        job_data: Job containing character_names, player_names, prompt_template\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    segments = job_data['segments']\n",
    "    idx = segment_data['index']\n",
    "    \n",
    "    prev_text = segments[idx-1]['text'] if idx > 0 else \"\"\n",
    "    current_text = segment_data['text']\n",
    "    next_text = segments[idx+1]['text'] if idx < len(segments) - 1 else \"\"\n",
    "    \n",
    "    char_list = \", \".join(job_data['character_names']) if job_data['character_names'] else \"Unknown\"\n",
    "    player_list = \", \".join(job_data['player_names']) if job_data['player_names'] else \"Unknown\"\n",
    "    \n",
    "    return job_data['prompt_template'].format(\n",
    "        char_list=char_list,\n",
    "        player_list=player_list,\n",
    "        prev_text=prev_text,\n",
    "        current_text=current_text,\n",
    "        next_text=next_text\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_classification_response(response: str, index: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse model response into classification result.\n",
    "    \n",
    "    Expected format:\n",
    "    Classificatie: IC|OOC|MIXED\n",
    "    Reden: <reasoning>\n",
    "    Vertrouwen: <0.0-1.0>\n",
    "    Personage: <name or N/A>\n",
    "    \n",
    "    Args:\n",
    "        response: Model response text\n",
    "        index: Segment index\n",
    "    \n",
    "    Returns:\n",
    "        Classification result dictionary\n",
    "    \"\"\"\n",
    "    # Default values\n",
    "    classification = \"IC\"\n",
    "    confidence = 0.7\n",
    "    reasoning = \"Could not parse response\"\n",
    "    character = None\n",
    "    \n",
    "    # Extract classification\n",
    "    class_match = re.search(r'Classificatie:\\s*(\\w+)', response, re.IGNORECASE)\n",
    "    if class_match:\n",
    "        classification = class_match.group(1).strip().upper()\n",
    "    \n",
    "    # Extract reasoning\n",
    "    reden_match = re.search(\n",
    "        r'Reden:\\s*(.+?)(?=(?:Vertrouwen:|Personage:|$))',\n",
    "        response,\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    if reden_match:\n",
    "        reasoning = reden_match.group(1).strip()\n",
    "    \n",
    "    # Extract confidence\n",
    "    conf_match = re.search(r'Vertrouwen:\\s*([\\d.]+)', response, re.IGNORECASE)\n",
    "    if conf_match:\n",
    "        try:\n",
    "            confidence = float(conf_match.group(1).strip())\n",
    "            confidence = max(0.0, min(1.0, confidence))  # Clamp to [0, 1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Extract character\n",
    "    char_match = re.search(r'Personage:\\s*(.+?)(?:\\n|$)', response, re.IGNORECASE)\n",
    "    if char_match:\n",
    "        char_text = char_match.group(1).strip()\n",
    "        if char_text.upper() != \"N/A\":\n",
    "            character = char_text\n",
    "    \n",
    "    return {\n",
    "        \"segment_index\": index,\n",
    "        \"classification\": classification,\n",
    "        \"confidence\": confidence,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"character\": character\n",
    "    }\n",
    "\n",
    "\n",
    "def process_job(job_file: Path) -> None:\n",
    "    \"\"\"\n",
    "    Process a single classification job.\n",
    "    \n",
    "    Args:\n",
    "        job_file: Path to job JSON file\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {job_file.name}\")\n",
    "    \n",
    "    # Load job data\n",
    "    with open(job_file, 'r', encoding='utf-8') as f:\n",
    "        job_data = json.load(f)\n",
    "    \n",
    "    job_id = job_data['job_id']\n",
    "    segments = job_data['segments']\n",
    "    \n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"Segments to classify: {len(segments)}\")\n",
    "    \n",
    "    # Classify each segment\n",
    "    classifications = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment_with_index = {**segment, 'index': i}\n",
    "        prompt = build_prompt(segment_with_index, job_data)\n",
    "        \n",
    "        # Get classification from model\n",
    "        response = classify_segment(prompt)\n",
    "        result = parse_classification_response(response, i)\n",
    "        \n",
    "        classifications.append(result)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == len(segments):\n",
    "            print(f\"  Progress: {i+1}/{len(segments)} segments classified\")\n",
    "    \n",
    "    # Write results\n",
    "    result_file = Path(complete_dir) / f\"{job_id}_result.json\"\n",
    "    result_data = {\n",
    "        \"job_id\": job_id,\n",
    "        \"classifications\": classifications\n",
    "    }\n",
    "    \n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"[OK] Results written: {result_file.name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "print(\"[OK] Classification functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Start Classification Worker (Fixed for Google Drive)\nimport time\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nimport signal\nimport sys\n\nprint(\"[START] Classification worker\")\nprint(\"[INFO] Watching:\", pending_dir)\nprint(\"[INFO] Results to:\", complete_dir)\n\n# EXPLICIT DEBUG\nprint(\"\\n[DEBUG] Testing file detection:\")\nprint(f\"  pending_dir type: {type(pending_dir)}\")\nprint(f\"  pending_dir value: {pending_dir}\")\nprint(f\"  str(pending_dir): {str(pending_dir)}\")\ntry:\n    test_list = os.listdir(str(pending_dir))\n    print(f\"  os.listdir() returned: {test_list}\")\n    test_filtered = [f for f in test_list if f.startswith('job_') and f.endswith('.json')]\n    print(f\"  Filtered job files: {test_filtered}\")\nexcept Exception as e:\n    print(f\"  os.listdir() ERROR: {e}\")\n\nprint(\"\\nPress Ctrl+C (or interrupt kernel) to stop\\n\")\nprint(\"=\" * 60)\n\n# IMPORTANT: Reset processed jobs on each run to pick up existing files\nprocessed_jobs = set()\nstop_requested = False\n\n# Check for existing job files using os.listdir (works better with Google Drive)\ndef find_job_files(directory):\n    \"\"\"Find job files using os.listdir (more reliable with Google Drive)\"\"\"\n    try:\n        dir_str = str(directory)\n        all_files = os.listdir(dir_str)\n        job_files = [f for f in all_files if f.startswith('job_') and f.endswith('.json')]\n        return [Path(directory) / f for f in job_files]\n    except Exception as e:\n        print(f\"[ERROR] Could not list directory {directory}: {e}\")\n        import traceback\n        traceback.print_exc()\n        return []\n\ninitial_files = find_job_files(pending_dir)\nif initial_files:\n    print(f\"[INFO] Found {len(initial_files)} existing job(s) to process\")\n    for f in initial_files:\n        print(f\"  - {f.name} ({f.stat().st_size / 1024 / 1024:.1f} MB)\")\nelse:\n    print(\"[INFO] No existing jobs found, will wait for new ones\")\n\nprint()\n\ndef signal_handler(sig, frame):\n    global stop_requested\n    stop_requested = True\n    print(\"\\n[STOP] Stop requested, finishing current job...\")\n\n# Register signal handler\nsignal.signal(signal.SIGINT, signal_handler)\n\ntry:\n    while not stop_requested:\n        job_files = find_job_files(pending_dir)\n        new_jobs = [f for f in job_files if f.name not in processed_jobs]\n\n        if new_jobs:\n            print(f\"[{datetime.now():%H:%M:%S}] Processing {len(new_jobs)} job(s)\")\n            for job_file in new_jobs:\n                if stop_requested:\n                    print(\"[INFO] Skipping remaining jobs due to stop request\")\n                    break\n                try:\n                    process_job(job_file)\n                    processed_jobs.add(job_file.name)\n                    job_file.unlink()\n                except Exception as exc:\n                    print(f\"[ERROR] Failed on {job_file.name}: {exc}\")\n                    import traceback\n                    traceback.print_exc()\n        else:\n            # Only print waiting message occasionally to reduce spam\n            if len(processed_jobs) == 0 or int(time.time()) % 30 == 0:\n                print(f\"[{datetime.now():%H:%M:%S}] No pending jobs, waiting...\")\n\n        # Sleep in smaller intervals to check stop_requested more frequently\n        for _ in range(10):\n            if stop_requested:\n                break\n            time.sleep(0.5)\n\nexcept Exception as e:\n    print(f\"\\n[ERROR] Worker crashed: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    print(\"\\n[STOP] Worker stopped\")\n    print(f\"[INFO] Processed {len(processed_jobs)} jobs total\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}