{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D&D Session IC/OOC Classification Worker\n",
    "\n",
    "This notebook runs on Google Colab to provide GPU-accelerated classification for the VideoChunking pipeline.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Open in Colab**: `File ‚Üí Open in Colab` or upload to Google Drive\n",
    "2. **Enable GPU**: `Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU ‚Üí T4`\n",
    "3. **Mount Google Drive**: Run Cell 1\n",
    "4. **Install Dependencies**: Run Cell 2\n",
    "5. **Load Model**: Run Cell 3 (this may take a few minutes)\n",
    "6. **Start Worker**: Run Cell 4 - this will continuously process jobs\n",
    "\n",
    "## How It Works\n",
    "\n",
    "- Your local pipeline uploads classification jobs to `VideoChunking/classification_pending/` in Google Drive\n",
    "- This notebook watches that folder and processes jobs using a local LLM\n",
    "- Results are written to `VideoChunking/classification_complete/`\n",
    "- Your local pipeline polls for results and continues\n",
    "\n",
    "Keep this notebook running while processing sessions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-320320814.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create classification directories if they don't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Cell 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create classification directories if they don't exist\n",
    "pending_dir = '/content/drive/MyDrive/VideoChunking/classification_pending'\n",
    "complete_dir = '/content/drive/MyDrive/VideoChunking/classification_complete'\n",
    "\n",
    "os.makedirs(pending_dir, exist_ok=True)\n",
    "os.makedirs(complete_dir, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Google Drive mounted\")\n",
    "print(f\"‚úì Pending jobs: {pending_dir}\")\n",
    "print(f\"‚úì Completed jobs: {complete_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "!pip install -q transformers torch accelerate bitsandbytes\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Load LLM Model\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Model selection - Qwen2.5-3B-Instruct (fits entirely on free T4 GPU)\n# Smaller but still excellent for classification tasks\nMODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n\nprint(f\"Loading model: {MODEL_NAME}\")\nprint(\"This may take 2-5 minutes on first run...\")\n\n# Configure 8-bit quantization to fit on free GPU\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load model with 8-bit quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Print memory usage\nif torch.cuda.is_available():\n    print(f\"‚úì Model loaded on GPU: {model.device}\")\n    print(f\"‚úì GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\nelse:\n    print(f\"‚úì Model loaded on CPU: {model.device}\")\n    print(f\"‚ö† Warning: Running on CPU will be slower. Consider enabling GPU in Runtime settings.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Classification Functions\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "def classify_segment(prompt: str, max_length: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Classify a single segment using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Classification prompt\n",
    "        max_length: Maximum tokens for generation\n",
    "    \n",
    "    Returns:\n",
    "        Model response text\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def build_prompt(segment_data: Dict, job_data: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Build classification prompt from segment and job data.\n",
    "    \n",
    "    Args:\n",
    "        segment_data: Current segment with text\n",
    "        job_data: Job containing character_names, player_names, prompt_template\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    segments = job_data['segments']\n",
    "    idx = segment_data['index']\n",
    "    \n",
    "    prev_text = segments[idx-1]['text'] if idx > 0 else \"\"\n",
    "    current_text = segment_data['text']\n",
    "    next_text = segments[idx+1]['text'] if idx < len(segments) - 1 else \"\"\n",
    "    \n",
    "    char_list = \", \".join(job_data['character_names']) if job_data['character_names'] else \"Unknown\"\n",
    "    player_list = \", \".join(job_data['player_names']) if job_data['player_names'] else \"Unknown\"\n",
    "    \n",
    "    return job_data['prompt_template'].format(\n",
    "        char_list=char_list,\n",
    "        player_list=player_list,\n",
    "        prev_text=prev_text,\n",
    "        current_text=current_text,\n",
    "        next_text=next_text\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_classification_response(response: str, index: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse model response into classification result.\n",
    "    \n",
    "    Expected format:\n",
    "    Classificatie: IC|OOC|MIXED\n",
    "    Reden: <reasoning>\n",
    "    Vertrouwen: <0.0-1.0>\n",
    "    Personage: <name or N/A>\n",
    "    \n",
    "    Args:\n",
    "        response: Model response text\n",
    "        index: Segment index\n",
    "    \n",
    "    Returns:\n",
    "        Classification result dictionary\n",
    "    \"\"\"\n",
    "    # Default values\n",
    "    classification = \"IC\"\n",
    "    confidence = 0.7\n",
    "    reasoning = \"Could not parse response\"\n",
    "    character = None\n",
    "    \n",
    "    # Extract classification\n",
    "    class_match = re.search(r'Classificatie:\\s*(\\w+)', response, re.IGNORECASE)\n",
    "    if class_match:\n",
    "        classification = class_match.group(1).strip().upper()\n",
    "    \n",
    "    # Extract reasoning\n",
    "    reden_match = re.search(\n",
    "        r'Reden:\\s*(.+?)(?=(?:Vertrouwen:|Personage:|$))',\n",
    "        response,\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    if reden_match:\n",
    "        reasoning = reden_match.group(1).strip()\n",
    "    \n",
    "    # Extract confidence\n",
    "    conf_match = re.search(r'Vertrouwen:\\s*([\\d.]+)', response, re.IGNORECASE)\n",
    "    if conf_match:\n",
    "        try:\n",
    "            confidence = float(conf_match.group(1).strip())\n",
    "            confidence = max(0.0, min(1.0, confidence))  # Clamp to [0, 1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Extract character\n",
    "    char_match = re.search(r'Personage:\\s*(.+?)(?:\\n|$)', response, re.IGNORECASE)\n",
    "    if char_match:\n",
    "        char_text = char_match.group(1).strip()\n",
    "        if char_text.upper() != \"N/A\":\n",
    "            character = char_text\n",
    "    \n",
    "    return {\n",
    "        \"segment_index\": index,\n",
    "        \"classification\": classification,\n",
    "        \"confidence\": confidence,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"character\": character\n",
    "    }\n",
    "\n",
    "\n",
    "def process_job(job_file: Path) -> None:\n",
    "    \"\"\"\n",
    "    Process a single classification job.\n",
    "    \n",
    "    Args:\n",
    "        job_file: Path to job JSON file\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {job_file.name}\")\n",
    "    \n",
    "    # Load job data\n",
    "    with open(job_file, 'r', encoding='utf-8') as f:\n",
    "        job_data = json.load(f)\n",
    "    \n",
    "    job_id = job_data['job_id']\n",
    "    segments = job_data['segments']\n",
    "    \n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"Segments to classify: {len(segments)}\")\n",
    "    \n",
    "    # Classify each segment\n",
    "    classifications = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment_with_index = {**segment, 'index': i}\n",
    "        prompt = build_prompt(segment_with_index, job_data)\n",
    "        \n",
    "        # Get classification from model\n",
    "        response = classify_segment(prompt)\n",
    "        result = parse_classification_response(response, i)\n",
    "        \n",
    "        classifications.append(result)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == len(segments):\n",
    "            print(f\"  Progress: {i+1}/{len(segments)} segments classified\")\n",
    "    \n",
    "    # Write results\n",
    "    result_file = Path(complete_dir) / f\"{job_id}_result.json\"\n",
    "    result_data = {\n",
    "        \"job_id\": job_id,\n",
    "        \"classifications\": classifications\n",
    "    }\n",
    "    \n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úì Results written: {result_file.name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "print(\"‚úì Classification functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Start Classification Worker\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ Starting classification worker...\")\n",
    "print(\"üìÅ Watching:\", pending_dir)\n",
    "print(\"üì§ Results to:\", complete_dir)\n",
    "print(\"\\nPress Ctrl+C (or interrupt kernel) to stop\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Track processed jobs to avoid reprocessing\n",
    "processed_jobs = set()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Find pending jobs\n",
    "        pending_path = Path(pending_dir)\n",
    "        job_files = list(pending_path.glob(\"job_*.json\"))\n",
    "        \n",
    "        # Filter out already processed jobs\n",
    "        new_jobs = [f for f in job_files if f.name not in processed_jobs]\n",
    "        \n",
    "        if new_jobs:\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Found {len(new_jobs)} new job(s)\")\n",
    "            \n",
    "            for job_file in new_jobs:\n",
    "                try:\n",
    "                    process_job(job_file)\n",
    "                    processed_jobs.add(job_file.name)\n",
    "                    \n",
    "                    # Delete processed job file\n",
    "                    job_file.unlink()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing {job_file.name}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "        else:\n",
    "            # No new jobs, wait\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] No pending jobs, waiting...\")\n",
    "        \n",
    "        # Sleep before next check\n",
    "        time.sleep(5)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nüõë Worker stopped by user\")\n",
    "    print(f\"Processed {len(processed_jobs)} jobs total\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}